# ğŸ“˜ **Technical Architecture & Implementation Guide**

This document describes the architecture, components, data flow, and implementation of your MCP-enabled proxy system, intended for developers, architects, and operations engineers.

---

## ğŸ§  **1. Overview**

Your system enables clients (e.g., Postman, ServiceNow) to send simple HTTP requests containing prompts or queries, and receive *structured, context-rich answers* generated by an LLM (Claude), augmented by tool invocations exposed via the Model Context Protocol (MCP).
The solution abstracts away the complexity of MCP and provides a RESTful endpoint for ease of integration.

At a high level, the flow is:

```plaintext
Client HTTP Request
  â†“
Proxy Server (Express + ngrok)
  â†“
MCP Client (long-lived)
  â†“
MCP Server (weather tools)
  â†“
External API (NWS)
  â†“
MCP returns result â†’ Proxy â†’ Client
```

This aligns with MCPâ€™s client-server architecture, where:

* **MCP Client** manages LLM interactions and tool invocation logistics.
* **MCP Server** exposes structured tools to the MCP Client via standardized interfaces. ([Model Context Protocol][1])

---

## ğŸ§± **2. System Components**

### ğŸ“Œ 2.1 Proxy Server

**Role:**
Acts as the external entry point. Accepts HTTP requests from clients and orchestrates calls to the MCP client.

**Tech Stack:**

* Node.js
* Express.js (HTTP framework)
* ngrok SDK (public HTTPS exposure)
* dotenv (environment variable loading)

**Key Responsibilities:**

* Listen for `POST /prompt`
* Initialize and maintain a persistent MCP Client instance
* Translate HTTP prompts to MCP client calls
* Return JSON responses
* Expose the local proxy to the internet via ngrok

**Why this is needed:**
Standard web clients (like Postman or ServiceNow) donâ€™t understand MCP protocol directly; the proxy translates between plain REST and MCP client usage, so your internal implementation remains invisible to external systems. ([Markaicode][2])

---

### ğŸ“Œ 2.2 MCP Client

**Role:**
Encapsulates:

* LLM interactions via the Anthropic SDK
* Communication with the MCP Server using the MCP protocol over STDIO transport
* Tool invocation logic and result assembly

It is long-lived (initialized once at server startup) for performance and reuse.

**Mechanics:**

* Connects to the MCP server using `StdioClientTransport`
* Discovers available tools
* For each prompt:

  1. Sends input to Claude
  2. Interprets Claudeâ€™s responses
  3. Calls tools if needed
  4. Continues conversation
  5. Produces final combined text

This LLM + tool integration pattern is central to MCPâ€™s modular design. ([Kubiya.ai][3])

---

### ğŸ“Œ 2.3 MCP Server

**Role:**
Exposes capabilities (tools) such as forecast and alerts via a standardized MCP interface. This server runs independently of the proxy.

**Functions:**

* Register tools (e.g., `get-forecast`, `get-alerts`)
* Accept STDIO connections from one or more MCP clients
* Execute tools and respond with results

**Mechanics:**

* Each tool is defined with a name, input schema, description, and resolver function
* Resolver functions fetch external data (e.g., NWS weather API)
* Return data structured as `content` arrays used by the MCP client

This serves as a *modular backend for LLMs* that can be extended with more capabilities over time. ([Model Context Protocol][1])

---

### ğŸ“Œ 2.4 External API

Your weather tool makes HTTP requests to the U.S. National Weather Service (NWS) endpoints to fetch point and forecast data. This is a real external API integrated into the MCP server tools.

---

## ğŸ”„ **3. End-to-End Request Flow**

Letâ€™s break down the complete journey of a prompt from the client to response:

---

### ğŸ“Œ Step 1 â€” Client Sends Prompt

Client (e.g., Postman) sends:

```http
POST /prompt
Content-Type: application/json

{
  "prompt": "What is the weather in Sacramento today?"
}
```

---

### ğŸ“Œ Step 2 â€” Proxy Server Receives Request

* Express parses the JSON body.
* A log is printed (for debugging/observability).
* The proxy checks if the MCP client is initialized.
* If not, returns an HTTP error; otherwise proceeds.

---

### ğŸ“Œ Step 3 â€” Proxy â†’ MCP Client

The proxy invokes:

```js
const answer = await mcpClient.processQuery(prompt);
```

This call triggers **MCP client logic**.

---

### ğŸ“Œ Step 4 â€” MCP Client Internals

**4.1 Invoke LLM:**

```js
const response = await this.anthropic.messages.create({...});
```

This asks Claude what to do next. If Claude wants to use a tool, it returns a tool invocation.

**4.2 Tool Use:**
If LLM decides:

```json
{"type":"tool_use","name":"get-forecast","input":{"latitude":38.5816,"longitude":-121.4944}}
```

MCP client triggers `this.mcp.callTool(...)`, where `this.mcp` is the MCP SDK client. This sends a JSON-RPC request to the MCP server via STDIO.

**4.3 Tool Execution:**
The MCP server receives the request and executes the registered tool code (fetching from NWS), then returns structured weather data.

**4.4 Continue Conversation:**
The MCP client may send results back to the LLM for further refinement. All outcomes are concatenated into a final string.

This entire exchange is part of MCPâ€™s standardized client-server messaging. ([MCP Protocol][4])

---

### ğŸ“Œ Step 5 â€” Proxy Responds

The final text is returned to the proxy, which wraps it in:

```json
{ "response": "<final text>" }
```

and sends it back to the clientâ€™s HTTP connection.

---

## ğŸ›  **4. Implementation Details (Code Walkthrough)**

### ğŸ“Œ Proxy Server (`index.js`)

```js
require("dotenv").config();   // Load .env first
const express = require("express");
const http = require("http");
const ngrok = require("@ngrok/ngrok");
const MCPClient = require("./mcpClient.js");   // Import built MCP client

let mcpClient;

// Initialize long-lived MCP client
async function initMCP() {
  mcpClient = new MCPClient();
  await mcpClient.connectToServer("/path/to/mcpServer.js");
  console.log("âœ… Proxy connected MCP client to server");
}

const app = express();
app.use(express.json());

app.post("/prompt", async (req, res) => {
  const { prompt } = req.body;
  try {
    const answer = await mcpClient.processQuery(prompt);
    res.json({ response: answer });
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});

// Start server + ngrok
const server = http.createServer(app);
server.listen(4000, async () => {
  console.log("Proxy started on 4000");
  await initMCP();
  const listener = await ngrok.connect({ addr: 4000, authtoken_from_env:true });
  console.log(`ğŸš€ ngrok tunnel: ${listener.url()}`);
});
```

This structure ensures:

* The proxy is reachable externally
* MCP client remains alive across requests
* API key for Claude is loaded via dotenv

---

### ğŸ“Œ MCP Client (your class)

Your client orchestrates:

* LLM requests and tool usage logic
* Maintains codec and protocol details
* Abstraction over the MCP SDK

---

### ğŸ“Œ MCP Server (weather)

This script registers tools like `get-forecast` and `get-alerts`:

* Wraps external API calls in `fetch(...)`
* Returns content arrays interpreted by MCP

---

## ğŸ” **5. Security & Best Practices**

### âš ï¸ API Key Handling

Your Anthropic API key is loaded via a `.env` file, which is appropriate for local or development but should be replaced with a secure secret manager in production.

Example secure alternatives:

* AWS Secrets Manager
* HashiCorp Vault
* Kubernetes Secrets
* Azure Key Vault

Avoid committing any keys to source control.

---

### ğŸ§  Proxy Hardening

Production proxies often include:

âœ… API key validation on incoming requests
âœ… Rate limiting (protect backend & budget)
âœ… JSON size limits
âœ… Logging/observability (structured logs + tracing)
âœ… Metrics (request/response times, error rates)

Tools such as Kong, Tyk, or a custom API gateway support these policies. ([Tyk API Management][5])

---

### ğŸ“ˆ Scalability Notes

If your MCP client or server needs to scale, you may:

* Horizontal scale the proxy behind a load balancer
* Maintain multiple MCP clients and distribute requests (pooling)
* Move to asynchronous task queues for heavy workloads
* Add circuit breakers for external API failure modes

---

## ğŸ§ª **6. Testing & Validation**

### Local Tests

Run MCP server and proxy locally:

```bash
node mcpServer.js
node index.js
```

Then:

```bash
curl -X POST http://localhost:4000/prompt \
-H "Content-Type: application/json" \
-d '{"prompt":"What is the weather in Sacramento?"}'
```

Should return structured forecast.

### Public Tests

With ngrok:

```bash
curl https://<ngrok-url>/prompt \
-H "Content-Type: application/json" \
-d '{"prompt":"weather Sacramento"}'
```

---

## ğŸ“Š **7. Why This Architecture Works**

* **Modularity:** Tool logic is isolated in the MCP server, independent of LLM logic. ([Model Context Protocol][1])
* **Reusability:** The MCP client can connect to multiple servers if needed. ([Kubiya.ai][3])
* **Abstraction:** External clients donâ€™t need to understand MCP, JSON-RPC, or internal protocols.
* **Extensibility:** You can add more tools without changing your proxy interface.

---

## ğŸ§© Summary

| Layer                       | Responsibility                                    |
| --------------------------- | ------------------------------------------------- |
| Client (Postman/ServiceNow) | Sends simple HTTP prompt                          |
| Proxy                       | Translates HTTP â†’ MCP, manages long-lived client  |
| MCP Client                  | Manages LLM interaction and tool invocation logic |
| MCP Server                  | Exposes domain tools (weather, alerts) via MCP    |
| External API                | Supplies real world data such as forecasts        |

